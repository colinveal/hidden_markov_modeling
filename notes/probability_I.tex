\chapter{Probability I}
\label{probability_I}

The notion of probability is a cornestore in Statistics and machine learning algorithms we will
review further in this text. This chapter introduces core concepts from the theory of probability.
The following chapter discusses the topic of probability distributions 

\section{Random Variables}
\label{random_variables}

In very simple wrods, a random or stochastic variable, say $X$, is a variable for which we cannot
say with certainty what its value will be when performing an experiment that affects this variable.
Let's see an example to understand this.


\begin{framed}
\begin{example}

Let's consider that the variable $X$ represents the outcome of rolling a dice. Thus, $X$ takes values
in the range $\{1, 2, 3, 4, 5, 6\}$. Unless you have some knowledge about the dice itself and the 
detailed physics that occurs when we role the dice, you cannot predict what the outcome of the experiment 
will be. In this sense, $X$ is a random variable. In fact, it is a discrete random variable. Furtheremore,
if we assume that the dice is fair, we can also say that

\begin{equation}
P(X = i) = \frac{1}{6}, i=1, 2,3,4,5,6.
\end{equation}

As an asside, the only thing we can say with certainty is that the value of $X$ will in the range from one to six.
\end{example}
\end{framed}


\begin{framed}
\begin{remark}

We will be using the notation 

\begin{equation} 
\{X = x_i \} \nonumber
\end{equation}

to indicate the fact that the random variable $X$ assumes the value $x_i$ 
\end{remark}
\end{framed}

We can designate $P(X = i)$ via a function $f_X (x_i)$. We can then introduce the notion of a discrete probability distribution as the tuple

\begin{equation} 
f_X = (f_X(x_1),\dots, f_X(x_6))  
\end{equation}

For $f_X$ to be a probability distribution we should have

\begin{eqnarray}
f_X(x_i) \ge 0 \\
\sum_{i} f_X(x_i) = 1 
\end{eqnarray} 


\begin{framed}
\begin{example}

In Java we can generate values for the uniform distribution using the \mintinline{c++}{java.util.Random}.
Here is the sample code.

\begin{minted}{c++}
\end{minted}

\end{example}
\end{framed}


\subsubsection{Predict}

At this step an estimate of both the state vector $\mathbf{x}$ and the covariance matrix $\mathbf{P}$ is made.
This is done according to

\begin{equation}
\bar{\mathbf{x}}_k = \mathbf{f}(\hat{\mathbf{x}}_{k-1}, \mathbf{u}_k, \mathbf{w}_k)
\end{equation}

where $\mathbf{f}$ is described by equations \ref{equ1},  \ref{equ2} and \ref{equ3}. $\hat{\mathbf{x}}_{k-1}$ is the state at the previous
time step. $\mathbf{u}_k, \mathbf{w}$ are the input vector and error vector associated with the process. The covariance matrix is estimated via 

\begin{equation}
\bar{\mathbf{P}}_k = \mathbf{F}_k \mathbf{P}_{k-1} \mathbf{F}_{k}^T + \mathbf{L}_k\mathbf{Q}_k\mathbf{L}_{k}^T
\end{equation}

where $\mathbf{F}$ is the Jacobian matrix of $\mathbf{f}$ with respect to the state variables. 
$\mathbf{Q}_k$ is the covariance matrix of the error and $\mathbf{L}_k$ is the Jacobian matrix of the motion
model, i.e. $\mathbf{f}$, with respect to $\mathbf{w}$.

\subsubsection{Update}

The update step established the predicted state vector and covariance matrix. Overall this step is summarized by
the equations below

\begin{equation}
\mathbf{S}_k = \mathbf{H}_k \bar{\mathbf{P}}_k \mathbf{H}_{k}^T + \mathbf{M}_k\mathbf{R}_k\mathbf{M}_{k}^T
\end{equation}

\begin{equation}
\mathbf{K}_k = \bar{\mathbf{P}}_k \mathbf{H}_{k}^T\mathbf{S}_{k}^{-1}
\label{gain_matrix} 
\end{equation} 

\begin{equation}
\mathbf{x}_{k} = \bar{\mathbf{x}}_{k} + \mathbf{K}(\mathbf{z}_k - \mathbf{h}(\bar{\mathbf{x}}_k, \mathbf{v}_k))
\end{equation}

\begin{equation}
\mathbf{P}_k = (\mathbf{I} - \mathbf{K}_k\mathbf{H}_k)\bar{\mathbf{P}}_k
\end{equation}

where $\mathbf{H}$ is the Jacobian matrix of the observation model $\mathbf{h}$. $\mathbf{M}$ is the Jacobian matrix of 
the observation model with respect to the error vector $\mathbf{v}$.  $\mathbf{K}$ is the
gain matrix and $\mathbf{R}$ is the covariance matrix related to the error vector $\mathbf{v}$.
 
